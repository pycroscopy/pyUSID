{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# pyUSID in 10 minutes\n",
    "\n",
    "**Rajiv Giridharagopal (University of Washington)**\n",
    "\n",
    "4/24/2020\n",
    "\n",
    "**This document serves as a quick primer to the essential components of\n",
    "pyUSID**\n",
    "\n",
    "\n",
    "## Recommended pre-requisite reading\n",
    "* [Universal Spectroscopic and Imaging Data (USID) model](https://pycroscopy.github.io/USID/usid_model.html)\n",
    "* [Crash course on HDF5 and h5py](./h5py_primer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sidpy\n",
    "import pyUSID as usid\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a 3D Matrix to USID\n",
    "\n",
    "As an example of how to use pyUSID to reformat data for analysis, let's take\n",
    "an image with some time-series at each pixel\n",
    "Let's imagine we have a 10 x 10 array that measures the height. But we're applying\n",
    "a voltage to oscillate the height at 2 Hz for 1 second, with 10 Hz sampling and a\n",
    "slight phase shift and amplitude shift at each pixel.\n",
    "\n",
    "In other words, a 10 x 10 x 100 array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data\n",
    "data = np.zeros((10,10,100))\n",
    "phase = np.reshape(np.linspace(-np.pi, np.pi, 100), [10,10])\n",
    "amp = np.reshape(np.linspace(1, 5, 100), [10,10])\n",
    "for x in np.arange(0,100):\n",
    "    for r in np.arange(data.shape[0]):\n",
    "        for c in np.arange(data.shape[1]):\n",
    "            data[r,c,x] = amp[r][c] * np.sin(2*np.pi * 2*x/100 + phase[r][c]) \n",
    "\n",
    "# To visualize a 3D stack, there's a handy function in sidpy\n",
    "_ = sidpy.plot_utils.plot_map_stack(data, num_comps=4, fig_mult=(2,10), pad_mult=(0.01,.4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of being 3D, we need it to be (10x10, 100) in 2D for USID. \n",
    "First, let's define the position dimensions, which are 10 x 10 nm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = usid.hdf_utils.build_ind_val_matrices([np.arange(0,10e-9, 1e-9), np.arange(0,10e-9, 1e-9)], \n",
    "                                             is_spectral=False)\n",
    "\n",
    "# pos[0] are the indices (0,1,2...), pos[1] are the values (0,1e-9, 2e-9...)\n",
    "# Second, let's define he spectral dimensions, which is our 1 s long waveform\n",
    "spec = usid.hdf_utils.build_ind_val_matrices([np.arange(0,1,0.01)], is_spectral=True)\n",
    "\n",
    "# Finally, we make our USID-compatible dataset, which is now (100, 100)\n",
    "data_reshape, _ = usid.hdf_utils.reshape_from_n_dims(data, pos[0], spec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a USID Dataset \n",
    "\n",
    "Now that we have a USID-compatible dataset, we're almost there\n",
    "Let's actually create the USID dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the USID Dimension to define the dimensions directly, including the units\n",
    "pos_dims = [usid.Dimension('Rows', 'm', np.arange(0,10e-9, 1e-9)),\n",
    "            usid.Dimension('Cols', 'm', np.arange(0,10e-9, 1e-9))]\n",
    "\n",
    "spec_dims = [usid.Dimension('Time', 's', np.arange(0,1,0.01))]\n",
    "\n",
    "# Use the ArrayTranslator to create our file\n",
    "# Let's define our HDF55 file. This is the name of a file we'll write into\n",
    "h5_path = 'cookbook_data.h5'\n",
    "\n",
    "# Now let's use the ArrayTranslator to write our data to an HDF5 file\n",
    "tran = usid.io.numpy_translator.ArrayTranslator()\n",
    "tran.translate(h5_path, 'data', data_reshape, 'Height', 'm',\n",
    "               pos_dims, spec_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We're done. It's just a few lines of code to convert a random matrix\n",
    "of data into a USID format. We just need to know a little about our data.\n",
    "Now that our cookbook_data is full of delicious data, let's crack it open.\n",
    "\n",
    "We can open the file by using the h5py command\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = h5py.File(h5_path, mode='r+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``print_tree`` shows all the contents in this HDF5 file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidpy.hdf_utils.print_tree(h5_file, rel_paths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data are in the ``Raw_Data`` dataset. How do we extract our data?\n",
    "First let's print all the Main datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(usid.hdf_utils.get_all_main(h5_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of all the Main datasets. In this case there's only a single one. \n",
    "So we can access our data in two ways:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1:\n",
    "# Access a specific dataset in the list of Main datasets\n",
    "data_usid = usid.hdf_utils.get_all_main(h5_file)[0]\n",
    "\n",
    "# Option 2:\n",
    "# We could highlight the path in the tree and access it directly\n",
    "data_usid = h5_file['Measurement_000/Channel_000/Raw_Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that reading the dataset in this manual manner only gives us the\n",
    "standard ``h5py.Dataset``:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_usid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's upgrade from ``h5py.Dataset`` to a ``USIDataset``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_usid = usid.USIDataset(data_usid)\n",
    "print(data_usid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a USID Dataset\n",
    "\n",
    "There are lots of advantages to this kind of dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look via the handy visualize option\n",
    "# What is the signal vs time at pixel (5,3)\n",
    "_, _ = data_usid.visualize(slice_dict={'Rows': 5, 'Cols': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access our data in the HDF5 dataset directly, we can use the ``[()]`` shortcut\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_form = data_usid[()]\n",
    "print(type(two_dim_form))\n",
    "print(two_dim_form.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are working on 3D dataset, we want reshape the flattened data\n",
    "present in the HDF5 dataset back to the original form of (10,10,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim_form = data_usid.get_n_dim_form()\n",
    "print(type(n_dim_form))\n",
    "print(n_dim_form.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the properties of our data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows = \\n{}'.format(data_usid.get_pos_values('Rows')))\n",
    "print('Cols = \\n{}'.format(data_usid.get_pos_values('Cols')))\n",
    "print('Time = \\n{}'.format(data_usid.get_spec_values('Time')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes of the data when it was written\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidpy.hdf_utils.get_attributes(data_usid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the path of the Main dataset within the HDF5 file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_usid.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the parent folder of this Dataset, you use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_usid.parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding some new data\n",
    "\n",
    "Let's say we process our data using some method and want to save that process.\n",
    "For the sake of argument, we'll just make a matrix that's the  magnitude^2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc = np.array(data_usid[()]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new group within this file to store our results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_group = usid.hdf_utils.create_indexed_group(h5_file[data_usid.parent.name], 'Magnitude')\n",
    "print(result_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"indexed\" part means it appends 000, 001, etc if we do this many times\n",
    "Many built-in pyUSID and pycroscopy command do this so we don't overwrite old\n",
    "results. Because of the power of HDF5 we can go back to old processing and see!\n",
    "\n",
    "There's an analogous command create_results_group if you'd like\n",
    "\n",
    "Anyway let's print our tree out for good measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidpy.hdf_utils.print_tree(h5_file, rel_paths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to add our data. But we want to add some attributes as well to describe\n",
    "what we've done to our data. Attributes are a dictionary, so let's create one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = {'Method': 'Magnitude_Squared', 'units': 'm^2'}\n",
    "sidpy.hdf_utils.write_simple_attrs(result_group, attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a new main dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_result = usid.hdf_utils.write_main_dataset(result_group, \n",
    "                                                data_proc, \n",
    "                                                'Data Squared', \n",
    "                                                'Height Squared', 'm^2', \n",
    "                                                pos_dims, \n",
    "                                                spec_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This populates our new folder with the new data! Let's look for the Magnitude folder in our tree:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidpy.hdf_utils.print_tree(h5_file, rel_paths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we get all the Main datasets, we see a new dataset pop up in our list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usid.hdf_utils.get_all_main(h5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to verify the attributes for the data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidpy.hdf_utils.get_attributes(data_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the data_group:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidpy.hdf_utils.get_attributes(data_result.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, to verify this dataset is a Main dataset (with position and spectral dimensions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidpy.hdf_utils.get_attributes(data_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
